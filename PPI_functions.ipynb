{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.20.3\n",
      "1.3.4\n",
      "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import recall_score, matthews_corrcoef, f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "#%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "import sys\n",
    "print(sys.version)\n",
    "import sklearn\n",
    "from scipy.stats import ttest_ind\n",
    "import scipy.stats  as stats\n",
    "#print(sklearn.__version__)\n",
    "from statsmodels.stats.weightstats import ztest as ztest\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import propy\n",
    "from propy import PyPro\n",
    "from propy.PyPro import GetProDes\n",
    "from pathlib import Path  \n",
    "import pandas as pd\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns list of all UNIQUE AMPs and their sequences from Angela's dataset\n",
    "def AMP_list(df1,file_name):\n",
    "    import pandas as pd\n",
    "    amp_list=[ ]\n",
    "    amp_seq=[ ]\n",
    "    file_name = open(file_name,\"a\")\n",
    "\n",
    "    for j in range(df1.shape[0]):\n",
    "        element1=df1['peptide I'].loc[j]\n",
    "        element2=df1['peptide II'].loc[j]\n",
    "        seq1=df1['seq I'].loc[j]\n",
    "        seq2=df1['seq II'].loc[j]\n",
    "        s1=seq1.replace(\" \", \"\")\n",
    "        s2=str(seq2).replace(\" \", \"\")\n",
    "        sq1=s1.replace(\"-\", \"\")\n",
    "        sq2=s2.replace(\"-\", \"\")\n",
    "     #print(type(s1),type(s2))\n",
    "\n",
    "        if type(element1) is not float and element1 not in amp_list:\n",
    "        #print(\"peptide I is:\", df1['peptide I'].loc[j],\", And seq is:\" , df1['seq I'].loc[j])\n",
    "        #print(\"peptide II is:\", df1['peptide II'].loc[j],\", And seq is:\" , df1['seq II'].loc[j])\n",
    "            amp_list.append(element1)\n",
    "            amp_seq.append(sq1)\n",
    "            file_name.write('\\n%s' % '>'+element1)\n",
    "            if pd.isnull(seq1) is False :\n",
    "                print(element1,sq1.upper())\n",
    "                file_name.write('\\n%s' % sq1.upper())\n",
    "\n",
    "        if type(element2) is not float and element2 not in amp_list:\n",
    "            amp_list.append(element2)\n",
    "            amp_seq.append(sq2)\n",
    "            file_name.write('\\n%s' % '>'+element2)\n",
    "            if pd.isnull(seq2) is False :\n",
    "                print(element2,sq2.upper())\n",
    "                file_name.write('\\n%s' % sq2.upper())\n",
    "\n",
    "            \n",
    "    file_name.close()\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a dictionary\n",
    "\n",
    "def AMP_pairs_dict(df1):\n",
    "    amp_list=[ ]\n",
    "    fic_list=[ ]\n",
    "    amp_dicts = {}\n",
    "    amp_dicts2 = {}\n",
    "    i=0\n",
    "    for j in range(df1.shape[0]):\n",
    "        element1=df1['peptide I'].loc[j]\n",
    "        element2=df1['peptide II'].loc[j]\n",
    "        element_fic=df1['FICI'].loc[j]\n",
    "        #print(element1, element2,element_fic)\n",
    " \n",
    "        if type(element1) is not float and element1 not in amp_list:\n",
    "            amp_list.append(element1)\n",
    "            if type(element2) is not float and element2 not in amp_list:\n",
    "                i=i+1\n",
    "                amp_list.append(element2)\n",
    "                #print(element1,element2)\n",
    "                amp_dicts[element1] = element2\n",
    "                amp_dicts2[element_fic] = [element1,element2]\n",
    "                #print(i)\n",
    "    return  amp_dicts,amp_dicts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amp_names(input_file):\n",
    "    n_data= []\n",
    "    myfile = open(input_file, 'r')\n",
    "    for line in myfile:\n",
    "        if  line.startswith('>'):\n",
    "            line=line[0:-1]\n",
    "            newstr = line.replace(\">\", \"\")\n",
    "            #print(newstr)\n",
    "            n_data.append(newstr)\n",
    "        \n",
    "    return n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euclidean_distance (df,AMP_A,AMP_B):\n",
    "    D_e=np.linalg.norm(df.loc[AMP_A] - df.loc[AMP_B])\n",
    "    #D_e=np.linalg.norm(AMP_A - AMP_B)\n",
    "\n",
    "\n",
    "    return D_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# this function returns a dataframe containing AMP pairs, metric distance and corresponding FIC\n",
    "def metric_amp(df,my_amps):\n",
    "    name_list2=df.index\n",
    "    list_fic=[]\n",
    "    list_d=[]\n",
    "    for x , y in my_amps[1].items():\n",
    "        if (y[0] in name_list2):\n",
    "            amp_distance=Euclidean_distance (df, y[0], y[1])\n",
    "            #print(x,y[0],y[1],amp_distance)\n",
    "            list_d.append(amp_distance)\n",
    "            list_fic.append(x)\n",
    "        \n",
    "        \n",
    "    return list_fic,list_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_charge(proseq):\n",
    "    \n",
    "    chargeDict = {\"A\":0, \"C\":0, \"D\":-1, \"E\":-1, \"F\":0, \"G\":0, \"H\":1, \"I\":0, \"K\":1, \"L\":0, \"M\":0, \"N\":0, \"P\":0, \"Q\":0, \"R\":1, \"S\":0, \"T\":0, \"V\":0, \"W\":0, \"Y\":0, \"X\":0}\n",
    "    netCharge = sum([chargeDict[x] for x in proseq])\n",
    "    #print(proseq, netCharge)\n",
    "    return netCharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standard_aa_fun(seq):\n",
    "    \n",
    "    chargeDict = {\"A\":0, \"C\":0, \"D\":-1, \"E\":-1, \"F\":0, \"G\":0, \"H\":1, \"I\":0, \"K\":1, \"L\":0, \"M\":0, \"N\":0, \"P\":0, \"Q\":0, \"R\":1, \"S\":0, \"T\":0, \"V\":0, \"W\":0, \"Y\":0}\n",
    "    standard_aa=list(chargeDict.keys())\n",
    "    val=1\n",
    "    for element in seq:\n",
    "        if element not in standard_aa:\n",
    "            val=0\n",
    "            #print(element,val)\n",
    "            break\n",
    "        \n",
    "    return val   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def All_Descriptors(input_file):\n",
    "    dfs = [] \n",
    "    df=[]\n",
    "    Name_list=[]\n",
    "    myfile = open(input_file, 'r')\n",
    "    i=0\n",
    "    for line in myfile:\n",
    "        if line.startswith('>'):\n",
    "            ll=line\n",
    "            ll=ll[0:-1]\n",
    "            newstr = ll.replace(\">\", \"\")\n",
    "        if not line.startswith('>'):\n",
    "            line=line[0:-1]\n",
    "            arg=standard_aa_fun(line)\n",
    "            if arg==1:\n",
    "                #print(\"seq\")\n",
    "                print(newstr)\n",
    "                Name_list.append(newstr)\n",
    "                i=i+1\n",
    "                Des=GetProDes(line)\n",
    "                alldes = Des.GetALL()\n",
    "                df = pd.DataFrame([alldes])\n",
    "                dfs.append(df)\n",
    "                ddf=pd.concat(dfs) \n",
    "                #print(Name_list)\n",
    "\n",
    "    myfile.close()     \n",
    "    #.reset_index(inplace=True) \n",
    "    return ddf,Name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_charge_calculator(input_file):\n",
    "    c_data= []\n",
    "    myfile = open(input_file, 'r')\n",
    "    \n",
    "    for line in myfile:\n",
    "        if line.startswith('>'):\n",
    "            ll=line\n",
    "            ll=ll[0:-1]\n",
    "            newstr = ll.replace(\">\", \"\")\n",
    "        if not line.startswith('>'):\n",
    "            line=line[0:-1]\n",
    "            arg=standard_aa_fun(line)\n",
    "            if arg==1:\n",
    "                #print(\"charge\")\n",
    "                #print(newstr)\n",
    "                line=line[0:-1]\n",
    "                net_c=total_charge(line)\n",
    "                c_data.append(net_c)\n",
    "            \n",
    "    return pd.DataFrame(c_data,columns=[\"net_charge\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Descriptors_complete(neg,csv_file):\n",
    "    nc_df=net_charge_calculator(neg)\n",
    "    all_df,amp_names_list=All_Descriptors(neg)\n",
    "    df_names=pd.DataFrame(amp_names_list,columns=[\"AMP_Name\"])\n",
    "    #print(df_names)\n",
    "    #print(nc_df)\n",
    "        \n",
    "    result1 = pd.merge(df_names, nc_df, left_index=True, right_index=True)\n",
    "    result = pd.merge(result1,all_df.reset_index(drop=True), left_index=True, right_index=True)\n",
    "    #print(result1)\n",
    "\n",
    "    filepath = Path(csv_file)  \n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "    result.to_csv(filepath)  \n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AMP_list2(df1,file_name):\n",
    "    import pandas as pd\n",
    "    amp_list=[ ]\n",
    "    amp_seq=[ ]\n",
    "    amp_dict1 = {}\n",
    "    amp_dict2 = {}\n",
    "\n",
    "    \n",
    "    file_name = open(file_name,'w')\n",
    "\n",
    "    for j in range(df1.shape[0]):\n",
    "        element1=df1['peptide'].loc[j]\n",
    "        seq1=df1['sequence'].loc[j]\n",
    "        s1=seq1.replace(\" \", \"\")\n",
    "        sq1=s1.replace(\"-\", \"\")\n",
    "        #print(j,element1)\n",
    "        if type(element1) is not float and element1 not in amp_list:\n",
    "            amp_list.append(element1)\n",
    "            amp_dict1[element1]=df1['MIC'].loc[j]\n",
    "            amp_dict2[element1]=df1['MIC_0'].loc[j]\n",
    "\n",
    "            amp_seq.append(sq1)\n",
    "            file_name.write('>' + element1 )\n",
    "            #file_name.write('\\n%s' % '>' + element1)\n",
    "            if pd.isnull(seq1) is False :\n",
    "                #print(element1,sq1.upper())\n",
    "                file_name.write('\\n%s' % sq1.upper() + '\\n' )\n",
    "\n",
    "       \n",
    "            \n",
    "    file_name.close()\n",
    "        \n",
    "    return amp_dict1,amp_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the original dataframe a returns a ready to use dataset\n",
    "def final_df(df,textfile,csvfile):\n",
    "\n",
    "    dict1,dict2=AMP_list2(df,textfile)\n",
    "    #print(dict1)\n",
    "    df1=Descriptors_complete(textfile, csvfile)\n",
    "    filepath = Path(csvfile)  \n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)      \n",
    "    #df1=pd.read_csv(csvfile, )\n",
    "    #df2=df1.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    df1['MIC']=list(dict1.values())\n",
    "    df1['MIC_0']=list(dict2.values())\n",
    "    \n",
    "    df1.to_csv(filepath)  \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_df(df):\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    df0=df.drop(columns=['AMP_Name'])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_normalized = scaler.fit_transform(df0)\n",
    "    scaled_features_df = pd.DataFrame(df_normalized, index=df0.index, columns=df0.columns)\n",
    "\n",
    "    return scaled_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the original dataframe a returns a ready to use dataset\n",
    "def final_df2(df,textfile,csvfile):\n",
    "\n",
    "    dict1,dict2=AMP_list0(df,textfile)\n",
    "    #print(dict1)\n",
    "    df1=Descriptors_complete(textfile, csvfile)\n",
    "    filepath = Path(csvfile)  \n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)      \n",
    "    #df1=pd.read_csv(csvfile, )\n",
    "    #df2=df1.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    df1['MIC']=list(dict1.values())\n",
    "    df1['MIC_0']=list(dict2.values())\n",
    "    \n",
    "    df1.to_csv(filepath)  \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AMP_list0(df1,file_name):\n",
    "    import pandas as pd\n",
    "    amp_list=[ ]\n",
    "    amp_seq=[ ]\n",
    "    amp_dict1 = {}\n",
    "    amp_dict2 = {}\n",
    "\n",
    "    \n",
    "    file_name = open(file_name,'w')\n",
    "\n",
    "    for j in range(df1.shape[0]):\n",
    "        #print(j)\n",
    "        element1=df1['peptide'].loc[j]\n",
    "        seq1=df1['sequence'].loc[j]\n",
    "        s1=seq1.replace(\" \", \"\")\n",
    "        sq1=s1.replace(\"-\", \"\")\n",
    "        #print(j,element1)\n",
    "        if type(element1) is not float and element1 not in amp_list:\n",
    "            amp_list.append(element1)\n",
    "            amp_dict1[element1]=df1['MIC'].loc[j]\n",
    "            amp_dict2[element1]=df1['MIC_0'].loc[j]\n",
    "\n",
    "            amp_seq.append(sq1)\n",
    "            file_name.write('>' + str(element1) )\n",
    "            #file_name.write('\\n%s' % '>' + element1)\n",
    "            if pd.isnull(seq1) is False :\n",
    "                #print(element1,sq1.upper())\n",
    "                file_name.write('\\n%s' % sq1.upper() + '\\n' )\n",
    "\n",
    "    file_name.close()\n",
    "        \n",
    "    return amp_dict1,amp_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "def cross_val_split(X,y,ns):\n",
    "    i = 0\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    Xtrain_cv = list(range(ns))\n",
    "    Xtest_cv = list(range(ns))\n",
    "    Ytrain_cv = list(range(ns))\n",
    "    Ytest_cv = list(range(ns))\n",
    "    cv = StratifiedShuffleSplit(n_splits=ns, test_size=0.2, random_state=999)\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        Xtrain_cv[i] = X[train_index]\n",
    "        Xtest_cv[i] = X[test_index]\n",
    "        Ytrain_cv[i] = y[train_index]\n",
    "        Ytest_cv[i] = y[test_index]\n",
    "        i+=1\n",
    "    return Xtrain_cv, Xtest_cv, Ytrain_cv, Ytest_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_SVM_grid_search(X,y,ns):\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 20)\n",
    "    model = LinearSVC()\n",
    "    C_range = np.logspace(-1,1,3)\n",
    "    param_grid = {\"C\": C_range}\n",
    "    scoring = ['accuracy']\n",
    "    kfold = StratifiedShuffleSplit(n_splits=ns, test_size=0.2, random_state=999)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid=param_grid,\n",
    "                           refit='accuracy',\n",
    "                           n_jobs=-1,\n",
    "                           cv=kfold,\n",
    "                           verbose=0)\n",
    "    grid_result = grid_search.fit(X_train,Y_train)\n",
    "    CC=grid_result.best_params_['C']\n",
    "    return CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_feature_selection(Xtrain_cv,Ytrain_cv,CC,ns,klist):\n",
    "    from sklearn.svm import LinearSVC\n",
    "    import numpy as np\n",
    "    listlocs = list(range(ns))\n",
    "    results = list(range(ns))\n",
    "    kfeatures = list(range(ns))\n",
    "    kweights = list(range(ns))\n",
    "\n",
    "    bagging_dict = {}\n",
    "    for i in range(ns): # loop over cross validation sets\n",
    "        model = LinearSVC()\n",
    "        params = {\"penalty\":\"l1\", \"C\":CC, \"loss\":\"squared_hinge\", \"dual\":False, \"max_iter\": 1000000}\n",
    "        model.set_params(**params)\n",
    "    \n",
    "        X_train = Xtrain_cv[i]\n",
    "        Y_train = Ytrain_cv[i]\n",
    "# coef_ is a list of weights same as w\n",
    "        model.fit(X_train, Y_train)\n",
    "        T = model.coef_[0]\n",
    "        TT = list(T)\n",
    "        locs = np.where(abs(T) > 0)\n",
    "        listlocs[i] = np.asarray(locs)\n",
    "\n",
    "        if i == 0:\n",
    "            flistlocs = listlocs[i]\n",
    "            flistlocs=flistlocs[0]\n",
    "        else:\n",
    "            flistlocs = np.intersect1d(flistlocs,listlocs[i])\n",
    "        \n",
    "\n",
    "        for f in flistlocs:\n",
    "            f_feature = klist[f]\n",
    "            f_weight = T[f]\n",
    "        \n",
    "            if i == 0:\n",
    "                bagging_dict[f_feature] = []\n",
    "                bagging_dict[f_feature].append(f_weight)\n",
    "            else:\n",
    "                bagging_dict[f_feature].append(f_weight)\n",
    "\n",
    "        kfeatures[i] = klist[listlocs[i]]\n",
    "        kweights[i] = T[listlocs[i]]\n",
    "        \n",
    "    kfeatures=klist[flistlocs]\n",
    "    featuredict={}\n",
    "    for k,v in bagging_dict.items():\n",
    "        if k in kfeatures:\n",
    "            print(k, sum(v)/len(v))\n",
    "            featuredict[k]=sum(v)/len(v)\n",
    "    return featuredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_feature_plot(featuredict,fig_title,fonts):\n",
    "    featuredict = sorted(featuredict.items(), key=lambda x:x[1])\n",
    "    feature_name=[]\n",
    "    feature_coef=[]\n",
    "\n",
    "    for items in featuredict:\n",
    "        #print(items[0], items[1])\n",
    "        feature_name.append(items[0])\n",
    "        feature_coef.append(items[1])\n",
    "    xx=feature_name\n",
    "    yy=feature_coef\n",
    "    # Plot of feature importance\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    plt.figure(figsize=(7, 7))\n",
    "    #sns.set_theme(style=\"whitegrid\")\n",
    "    ax = sns.barplot(yy, xx)\n",
    "    plt.xlabel(\"Feature Importance\",  fontsize=fonts)\n",
    "    \n",
    "    plt.savefig(fig_title, bbox_inches = 'tight',\n",
    "        pad_inches = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_grid_search(X,y):\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from numpy import arange\n",
    "    # define model\n",
    "    model = Lasso()\n",
    "    # define model evaluation method\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # define grid\n",
    "    grid = dict()\n",
    "    grid['alpha'] = arange(0, 1, 0.01)\n",
    "    # define search\n",
    "    search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "    # perform the search\n",
    "    results = search.fit(X, y)\n",
    "    AA = results.best_params_['alpha']\n",
    "    print(AA)\n",
    "    return AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_feature_selection(Xtrain_cv,Ytrain_cv,CC,ns,klist):\n",
    "    from sklearn.linear_model import Lasso\n",
    "    import numpy as np\n",
    "    listlocs = list(range(ns))\n",
    "    results = list(range(ns))\n",
    "    kfeatures = list(range(ns))\n",
    "    kweights = list(range(ns))\n",
    "\n",
    "    bagging_dict = {}\n",
    "    model = Lasso(alpha = CC)\n",
    "    for i in range(ns):\n",
    "    \n",
    "        X_train = Xtrain_cv[i]\n",
    "        Y_train = Ytrain_cv[i]\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "        #print(i)\n",
    "        #print(model.coef_)\n",
    "        T = model.coef_\n",
    "        #print(i)\n",
    "        #print(T)\n",
    "        TT = list(T)\n",
    "        locs = np.where(abs(T) > 0)\n",
    "        listlocs[i] = np.asarray(locs)\n",
    "\n",
    "        if i == 0:\n",
    "            flistlocs = listlocs[i]\n",
    "            flistlocs=flistlocs[0]\n",
    "        else:\n",
    "            flistlocs = np.intersect1d(flistlocs,listlocs[i])\n",
    "        \n",
    "\n",
    "        for f in flistlocs:\n",
    "            f_feature = klist[f]\n",
    "            f_weight = T[f]\n",
    "        \n",
    "            if i == 0:\n",
    "                bagging_dict[f_feature] = []\n",
    "                bagging_dict[f_feature].append(f_weight)\n",
    "            else:\n",
    "                bagging_dict[f_feature].append(f_weight)\n",
    "\n",
    "        kfeatures[i] = klist[listlocs[i]]\n",
    "        kweights[i] = T[listlocs[i]]\n",
    "        \n",
    "    kfeatures=klist[flistlocs]\n",
    "    featuredict={}\n",
    "    for k,v in bagging_dict.items():\n",
    "        if k in kfeatures:\n",
    "            print(k, sum(v)/len(v))\n",
    "            featuredict[k]=sum(v)/len(v)\n",
    "    return featuredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(X_train,Y_train):\n",
    "        #logistic regression\n",
    "        \n",
    "        log1=LogisticRegression(random_state=0)\n",
    "        \n",
    "        log1.fit(X_train,Y_train)\n",
    "        \n",
    "        \n",
    "        svc_model = SVC()\n",
    "        svc_model.fit(X_train, Y_train)\n",
    "\n",
    "        log = log1.score(X_train,Y_train)\n",
    "        svc = svc_model.score(X_train, Y_train)\n",
    "\n",
    "        y_pred_log=log1.predict(X_train)\n",
    "        y_pred_svc=svc_model.predict(X_train)\n",
    "\n",
    "        rec_log=recall_score(Y_train, y_pred_log, average='macro')\n",
    "        rec_svc=recall_score(Y_train, y_pred_svc, average='macro')\n",
    "        mat_log= matthews_corrcoef(Y_train, y_pred_log)\n",
    "        mat_svc= matthews_corrcoef(Y_train, y_pred_svc)\n",
    "        \n",
    "        f1_log=f1_score(Y_train, y_pred_log)\n",
    "        f1_svc=f1_score(Y_train, y_pred_svc)\n",
    "\n",
    "        #print('[0]logistic regression accuracy:',log)\n",
    "        #print('[1]SVM accuracy:',svc)\n",
    "        #print('[2]logistic regression recall:',rec_log)\n",
    "        #print('[3]SVM regression recall:',rec_svm)\n",
    "\n",
    "        #recall_score(y_true, y_pred, average='macro')\n",
    "        return log,svc,rec_log,rec_svc,mat_log,mat_svc,f1_log,f1_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_crossval(Xtrain_cv,Ytrain_cv,ns):\n",
    "    output_dict={}\n",
    "    ac_log_list = []\n",
    "    ac_svc_list = []\n",
    "    rec_log_list = []\n",
    "    rec_svc_list = []\n",
    "    mat_log_list = []\n",
    "    mat_svc_list = []\n",
    "    f1_log_list = []\n",
    "    f1_svc_list = []\n",
    "    \n",
    "    for i in range(ns):\n",
    "        log,svc,r_log,r_svc,m_log,m_svc,f1_log,f1_svc = models(Xtrain_cv[i],Ytrain_cv[i])\n",
    "        ac_log_list.append(log)\n",
    "        ac_svc_list.append(svc)\n",
    "        rec_log_list.append(r_log)\n",
    "        rec_svc_list.append(r_svc)\n",
    "        mat_log_list.append(m_log)\n",
    "        mat_svc_list.append(m_svc)\n",
    "        f1_log_list.append(m_log)\n",
    "        f1_svc_list.append(m_svc)\n",
    "        \n",
    "    ac_log_array = np.array(ac_log_list)\n",
    "    ac_svc_array = np.array(ac_svc_list)\n",
    "    r_log_array = np.array(rec_log_list)\n",
    "    r_svc_array = np.array(rec_svc_list)\n",
    "    m_log_array = np.array(mat_log_list)\n",
    "    m_svc_array = np.array(mat_svc_list)\n",
    "    f1_log_array = np.array(f1_log_list)\n",
    "    f1_svc_array = np.array(f1_svc_list)\n",
    "    \n",
    "    \n",
    "    ac_log_mean = np.mean(ac_log_array)\n",
    "    ac_svc_mean = np.mean(ac_svc_array)\n",
    "    r_log_mean = np.mean(r_log_array)\n",
    "    r_svc_mean = np.mean(r_svc_array)\n",
    "    m_log_mean = np.mean(m_log_array)\n",
    "    m_svc_mean = np.mean(m_svc_array) \n",
    "    f1_log_mean = np.mean(f1_log_array)\n",
    "    f1_svc_mean = np.mean(f1_svc_array)  \n",
    "        \n",
    "    #print('average logistic regression accuracy:'+str(ac_log_mean))\n",
    "    #print('average SVM accuracy:'+str(ac_svc_mean))\n",
    "    #print('average logistic regression recall:'+str(r_log_mean))\n",
    "    #print('average SVM recall:'+str(r_svc_mean))\n",
    "    #print('average logistic regression matthews_corrcoef:'+str(m_log_mean))\n",
    "    #print('average SVM matthews_corrcoef:'+str(m_svc_mean))\n",
    "    #print('average logistic regression f1 score:'+str(f1_log_mean))\n",
    "    #print('average SVM f1 score:'+str(f1_svc_mean))\n",
    "    \n",
    "    output_dict['average logistic regression accuracy:']=ac_log_mean\n",
    "    output_dict['average SVM accuracy:']=ac_svc_mean\n",
    "    output_dict['average logistic regression recall:']=r_log_mean\n",
    "    output_dict['average SVM recall:']=r_svc_mean\n",
    "    output_dict['average logistic regression matthews_corrcoef:']=m_log_mean\n",
    "    output_dict['average SVM matthews_corrcoef:']=m_svc_mean\n",
    "    output_dict['average logistic regression f1 score:']=f1_log_mean\n",
    "    output_dict['average SVM f1 score:']=f1_svc_mean\n",
    "    \n",
    "    return output_dict\n",
    "    #return ac_log_mean,ac_svc_mean,r_log_mean,r_svc_mean, m_log_mean, m_svc_mean, f1_log_mean, f1_svc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_function(X,y):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 20)\n",
    "    from sklearn.svm import SVC\n",
    "    svc_model = SVC()\n",
    "    svc_model.fit(X_train, y_train)\n",
    "    y_predict = svc_model.predict(X_test)\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    cm = np.array(confusion_matrix(y_test, y_predict, labels=[1,0]))\n",
    "    confusion = pd.DataFrame(cm, index=['AMP', 'non_AMP'],\n",
    "                         columns=['predicted_AMP','predicted_nonAMP'])\n",
    "    class_report = classification_report(y_test,y_predict)\n",
    "    print(class_report)\n",
    "    return svc_model,confusion,class_report,y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each bacterium we need X, y, and klist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary for each bacterium\n",
    "def bacterium_load(bacterium_file_name,data_dict,bacterium_name):\n",
    "    ddf=pd.read_csv(bacterium_file_name)\n",
    "    df2=ddf.drop(columns=['Unnamed: 0'])\n",
    "    df_norm=normalized_df(df2)\n",
    "    y = df2['MIC_0']\n",
    "    dff2 = df2.drop(columns=['AMP_Name','MIC','MIC_0'])\n",
    "    klist = np.array(dff2.columns)\n",
    "    #first z score\n",
    "    from scipy import stats\n",
    "\n",
    "    X = stats.zscore(dff2, axis = 1, ddof = 1, nan_policy = 'raise')\n",
    "    data_dict[bacterium_name] = list(range(3))\n",
    "    # also need: featuredict for \n",
    "    data_dict[bacterium_name][0] = X\n",
    "    data_dict[bacterium_name][1] = y\n",
    "    data_dict[bacterium_name][2] = klist\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redundant_filter(all_des):\n",
    "    redundant_cols = [ ]\n",
    "\n",
    "    for column1 in all_des:\n",
    "        for column2 in all_des:\n",
    "            if column1!=column2:\n",
    "                Cor_sp=stats.spearmanr(all_des[column1], all_des[column2])\n",
    "                if Cor_sp[0]>0.95:\n",
    "                    #print(column1,column2, Cor_sp[0] , Cor_sp[1] )\n",
    "                    if column1 not in redundant_cols:\n",
    "                        redundant_cols.append(column1)\n",
    "                        #print(redundant_cols)\n",
    "    return redundant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plotter(ddf2,feature_name,num):\n",
    "\n",
    "    from statsmodels.stats.weightstats import ztest as ztest\n",
    "    plt.style.use('classic')\n",
    "\n",
    "    plt.figure()\n",
    "    nonAMP_feature=ddf2[ddf2['MIC_0']==0][feature_name]\n",
    "    AMP_feature=ddf2[ddf2['MIC_0']==1][feature_name]\n",
    "    x1 = list(nonAMP_feature)\n",
    "\n",
    "    x2 = list(AMP_feature)\n",
    "    plt.hist(x2, density=True, bins=num, color='green', label='AMP')  # density=False would make counts\n",
    "    plt.hist(x1, density=True, bins=num, color='red', alpha = 0.5, label='nonAMP')  # density=False would make counts\n",
    "    \n",
    "    plt.ylabel('Frequency', fontsize = 20)\n",
    "    plt.xlabel(feature_name, fontsize = 20);\n",
    "    plt.legend(loc='upper right', fontsize =15)\n",
    "    plt.title((ztest(x1, x2, value=0) ))\n",
    "    plt.savefig('hist_' + feature_name + '.pdf')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_filter(ddf,seq_len):\n",
    "\n",
    "    ll_list= []\n",
    "    for i in range(len(ddf['sequence'])):\n",
    "    #print(i)\n",
    "        ll=len(ddf['sequence'].iloc[i])\n",
    "    #print(ll)\n",
    "        ll_list.append(ll)\n",
    "\n",
    "    df['seq_length']=ll_list\n",
    "    \n",
    "    return df[df['seq_length']>seq_len]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_ztest(df2):\n",
    "    import numpy as np\n",
    "    import pingouin as pg\n",
    "    feature_z = {}\n",
    "    i=0\n",
    "    for columns in df2:\n",
    "        nonAMP_feature=df2[df2['MIC_0']==0][columns]\n",
    "        AMP_feature=df2[df2['MIC_0']==1][columns]\n",
    "        x1 = list(nonAMP_feature)\n",
    "        x2 = list(AMP_feature)\n",
    "        #z_val=ztest(x1, x2, value=0) \n",
    "        z_val=pg.mwu(x1, x2, alternative='two-sided')\n",
    "        #if np.abs(z_val[0])>critical_z:\n",
    "        #    i+=1\n",
    "        print(columns, np.abs(z_val[0]),z_val[1],i)\n",
    "        #    feature_z[columns] = np.abs(z_val[0])\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mann_whitney_test(df1):\n",
    "\n",
    "#z_dict=feature_ztest(df1)\n",
    "    import numpy as np\n",
    "    import pingouin as pg\n",
    "    feature_u = []\n",
    "    \n",
    "    for columns in df1.columns:\n",
    "        nonAMP_feature=df1[df1['MIC_0']==0][columns]\n",
    "        AMP_feature=df1[df1['MIC_0']==1][columns]\n",
    "        x1 = list(nonAMP_feature)\n",
    "        x2 = list(AMP_feature)\n",
    "        #z_val=ztest(x1, x2, value=0) \n",
    "        z_val=pg.mwu(x1, x2, alternative='two-sided')\n",
    "        #tmp1 = np.float(z_val['RBC'])\n",
    "        #tmp2 = np.float(z_val['CLES'])\n",
    "        #print(columns)\n",
    "        #print(columns,z_val)\n",
    "        #print(columns,np.float(z_val['p-val']),np.float(z_val['U-val']),np.float(z_val['RBC']),np.float(z_val['CLES']))\n",
    "\n",
    "        if np.float(z_val['RBC'])<-0.3  and np.float(z_val['CLES'])>0.5:\n",
    "            print(columns,np.float(z_val['p-val']),np.float(z_val['U-val']),np.float(z_val['RBC']),np.float(z_val['CLES']))\n",
    "            feature_u.append(columns)\n",
    "        \n",
    "        \n",
    "    return feature_u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_csv(\"AT_descriptors2.csv\")\n",
    "def distance_PP(network_name):\n",
    "\n",
    "    df = pd.read_csv(network_name + \"_descriptors.csv\")\n",
    "    df1 = df.drop(columns = ['Unnamed: 0','AMP_Names'])\n",
    "    #df1.index = df['AMP_Name']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_normalized = scaler.fit_transform(df1)\n",
    "    scaled_features_df = pd.DataFrame(df_normalized, index=df1.index, columns=df1.columns)\n",
    "    scaled_features_df['AMP_Name']=df['AMP_Names']\n",
    "    scaled_features_df.set_index(\"AMP_Name\", inplace = True)\n",
    "    ll=df['AMP_Names'].tolist()\n",
    "    \n",
    "    with open(path_h+network_name+'_pair.txt') as file:\n",
    "        lines = [line.rstrip() for line in file]\n",
    "            \n",
    "        P1_list=[]\n",
    "        P2_list=[]\n",
    "        PPI_list=[]\n",
    "        PPI_dict={}\n",
    "        for items in lines:\n",
    "            x = items.split(\"\\t\")\n",
    "            #print(x)\n",
    "            if x[0] in ll:\n",
    "                if x[1] in ll:\n",
    "                    if x[2]=='+':\n",
    "                        PPI_dict[x[0],x[1]]=1\n",
    "                    else:\n",
    "                        PPI_dict[x[0],x[1]]=0    \n",
    "            \n",
    "    \n",
    "    # construct distance matrix for PPI\n",
    "    df_distance= pd.DataFrame()\n",
    "    df_distance['PPI_names']=list(PPI_dict.keys())\n",
    "    df_distance['PPI']=list(PPI_dict.values())\n",
    "\n",
    "    for columns in df1:\n",
    "        scaled_features_df2=scaled_features_df[columns]\n",
    "        xx=[]\n",
    "        for x , y in PPI_dict.items():\n",
    "            amp_distance=Euclidean_distance (scaled_features_df2, x[0], x[1])\n",
    "            xx.append(amp_distance)\n",
    "        df_distance[columns]=xx\n",
    "    #print(df_distance)\n",
    "    \n",
    "    df_distance.to_csv(network_name+'_distance.csv')\n",
    "    \n",
    "    return df_distance.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_features_plotter(featuredict,network_name,feature_selection_method,fonts,dr):\n",
    "\n",
    "    fig_title = network_name + \"_\" + feature_selection_method + \"_Feature_Importance.pdf\"\n",
    "    \n",
    "    if feature_selection_method == 'lasso':\n",
    "        bar_color = 'g'\n",
    "        sub_title='(a)'\n",
    "    elif feature_selection_method == 'svm':\n",
    "        bar_color = 'r'\n",
    "        sub_title='(b)'\n",
    "\n",
    "\n",
    "    featuredict = sorted(featuredict.items(), key=lambda x:x[1])\n",
    "    feature_name=[]\n",
    "    feature_coef=[]\n",
    "\n",
    "    for items in featuredict:\n",
    "        #print(items[0], items[1])\n",
    "        feature_name.append(items[0])\n",
    "        feature_coef.append(items[1])\n",
    "    xx=feature_name\n",
    "    yy=feature_coef\n",
    "    # Plot of feature importance\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    #plt.figure(figsize=(7, 7))\n",
    "    #dr=0.2\n",
    "    plt.barh(xx, yy, color=bar_color)\n",
    "    #ax.set_xticks(np.arange(-max(yy),max(yy),0.1))\n",
    "    #ax.set_xticks(np.arange(-np.round(max(yy),2),np.round(max(yy),2)+dr,dr))\n",
    "    ax.set_xticks(np.arange(np.round(min(yy),2),np.round(max(yy),2)+dr,dr))\n",
    "\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.xlabel(\"Feature Importance\",  fontsize=fonts)\n",
    "    plt.figtext(0.45,0.94,sub_title,size=25)\n",
    "    plt.tick_params(labelsize=8)\n",
    "    plt.savefig(fig_title, bbox_inches = 'tight',\n",
    "        pad_inches = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_vis(network_name,file_path):\n",
    "    import pandas as pd\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    with open(path_h + network_name+'_pair' +'.txt') as file:\n",
    "        lines = [line.rstrip() for line in file]\n",
    "    \n",
    "    P1_list=[]\n",
    "    P2_list=[]\n",
    "    PPI_list=[]\n",
    "    PPI_list_name=[]\n",
    "\n",
    "    for items in lines:\n",
    "        x = items.split(\"\\t\")\n",
    "        P1_list.append(x[0])\n",
    "        P2_list.append(x[1])\n",
    "        PPI_list.append(x[2])\n",
    "        PPI_list_name.append(x[0] + '-'+ x[1])\n",
    "\n",
    "    \n",
    "    df= pd.DataFrame()\n",
    "    df['Protein1']=P1_list\n",
    "    df['Protein2']=P2_list\n",
    "    df['PIP']=PPI_list\n",
    "    df['PIP_name']=PPI_list_name\n",
    "\n",
    "    df4=df[df.PIP=='+']\n",
    "    \n",
    "    \n",
    "    G4 = nx.from_pandas_edgelist(df4, source='Protein1', target='Protein2')\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    pos = nx.random_layout(G4)\n",
    "    nx.draw(G4,pos, ax=ax, with_labels=True,font_size=11,font_color  ='k')\n",
    "    fig.savefig(network_name+'_network.pdf', bbox_inches='tight')\n",
    " \n",
    "    return G4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, matthews_corrcoef, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "def models_overfitting(X_train, Y_train, X_test, Y_test):\n",
    "    # Feature scaling\n",
    "    #scaler = StandardScaler().fit(X_train)\n",
    "    #X_train_scaled = scaler.transform(X_train)\n",
    "    #X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize and train Logistic Regression\n",
    "    log1 = LogisticRegression(random_state=0).fit(X_train, Y_train)\n",
    "    \n",
    "    # Initialize and train SVM\n",
    "    svc_model = SVC().fit(X_train, Y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    log_metrics = evaluate_model(log1, X_train, Y_train, X_test, Y_test)\n",
    "    svc_metrics = evaluate_model(svc_model, X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "    # Print metrics\n",
    "    for metric, value in log_metrics.items():\n",
    "        print(f\"Logistic Regression {metric}: {value:.4f}\")\n",
    "    print(\"\\n\")\n",
    "    for metric, value in svc_metrics.items():\n",
    "        print(f\"SVC {metric}: {value:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Return as dictionaries for clarity\n",
    "    return {'logistic_regression': log_metrics, 'svc': svc_metrics}\n",
    "\n",
    "# Usage:\n",
    "# results = models_overfitting(X_train, Y_train, X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(network_name,path):\n",
    "    if path is None:\n",
    "        raise ValueError(\"The path is not specified.\")\n",
    "        \n",
    "    df0 = pd.read_csv(path+network_name + \"_distance.csv\")\n",
    "    df = df0.drop(columns = ['Unnamed: 0'])\n",
    "    df2=df.drop(columns=['PPI_names'])\n",
    "    df3=df.drop(columns=['PPI_names','PPI'])\n",
    "    X = np.array(df3)\n",
    "    y = np.array(df2['PPI'])\n",
    "    klist = np.array(df3.columns)\n",
    "    \n",
    "    return np.array(X), np.array(y), klist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, X_train, Y_train, X_test, Y_test):\n",
    "    \"\"\"Evaluate a model's performance on training and test data.\"\"\"\n",
    "    # Training predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    # Test predictions\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics calculation\n",
    "    metrics = {\n",
    "        'train_accuracy': accuracy_score(Y_train, y_train_pred),\n",
    "        'test_accuracy': accuracy_score(Y_test, y_test_pred),\n",
    "        'train_recall': recall_score(Y_train, y_train_pred, average='macro'),\n",
    "        'test_recall': recall_score(Y_test, y_test_pred, average='macro'),\n",
    "        'train_mcc': matthews_corrcoef(Y_train, y_train_pred),\n",
    "        'test_mcc': matthews_corrcoef(Y_test, y_test_pred),\n",
    "        'train_f1': f1_score(Y_train, y_train_pred),\n",
    "        'test_f1': f1_score(Y_test, y_test_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cross_species_evaluation(network1, network2, path):\n",
    "    \"\"\"\n",
    "    Train models on data from one organism (network1) and evaluate on another (network2).\n",
    "    \n",
    "    Parameters:\n",
    "    - network1: identifier or name for the first organism/network\n",
    "    - network2: identifier or name for the second organism/network\n",
    "    - path: directory path containing the datasets\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary containing performance metrics for both logistic regression and SVC models\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Load datasets\n",
    "    X_1, y_1, klist_1 = load_dataset(network1, path)\n",
    "    X_2, y_2, klist_2 = load_dataset(network2, path)\n",
    "    print(X_1.shape[1] , X_2.shape[1])\n",
    "\n",
    "    # Check if the feature dimensions match\n",
    "    if X_1.shape[1] != X_2.shape[1]:\n",
    "        logging.error(\"Feature dimensions of the two datasets do not match!\")\n",
    "        return None\n",
    "    \n",
    "    # Cross-validation split (consider using K-fold in the future)\n",
    "    ns = 1\n",
    "    Xtrain_1, Xtest_1, Ytrain_1, Ytest_1 = cross_val_split(X_1, y_1, ns)\n",
    "    # Train models\n",
    "    #log,svc,rec_log,rec_svc,mat_log,mat_svc,f1_log,f1_svc = models(Xtrain_1[0], Ytrain_1[0])\n",
    "\n",
    "    log_model = LogisticRegression(random_state=0).fit(Xtrain_1[0], Ytrain_1[0])\n",
    "    \n",
    "    svc_model = SVC().fit(Xtrain_1[0], Ytrain_1[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Evaluate models on network2 data\n",
    "    svc_metrics = evaluate_model(svc_model, Xtrain_1[0], Ytrain_1[0], X_2, y_2)\n",
    "    log_metrics = evaluate_model(log_model, Xtrain_1[0], Ytrain_1[0], X_2, y_2)\n",
    "\n",
    "    return {\n",
    "        'svc': svc_metrics,\n",
    "        'log': log_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_evaluate_xgboost(X_train, Y_train, X_test, Y_test):\n",
    "\n",
    "    # Initialize and train classifier\n",
    "    # Initialize and train classifier with hyperparameters to prevent overfitting\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=5,            # Typical values range from 3 to 10\n",
    "        min_child_weight=1,    # Increase to make the model more conservative\n",
    "        subsample=0.8,         # Value less than 1.0\n",
    "        colsample_bytree=0.9,  # Value less than 1.0\n",
    "        gamma=0.01,            # Increase to make the model more conservative\n",
    "        alpha=0.1,             # L1 regularization term (increase for more regularization)\n",
    "        reg_lambda=1               # L2 regularization term (increase for more regularization)\n",
    "    )       \n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate accuracy\n",
    "    #accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    xgb_metrics=evaluate_model(model, X_train, Y_train, X_test, Y_test)\n",
    "    \n",
    "    \n",
    "    return xgb_metrics\n",
    "\n",
    "\n",
    "\n",
    "def train_evaluate_xgboost2(X_train, Y_train, X_test, Y_test, colsample, gamma_value):\n",
    "    # Initialize and train classifier with specified hyperparameters\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=5,            # Typical values range from 3 to 10\n",
    "        min_child_weight=1,    # Increase to make the model more conservative\n",
    "        subsample=0.8,         # Value less than 1.0\n",
    "        colsample_bytree=colsample,  # Value less than 1.0\n",
    "        gamma=gamma_value,            # Increase to make the model more conservative\n",
    "        alpha=0.05,             # L1 regularization term (increase for more regularization)\n",
    "        reg_lambda=0.01               # L2 regularization term (increase for more regularization)\n",
    "    )   \n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    xgb_metrics=evaluate_model(model, X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "    \n",
    "    return xgb_metrics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_csv(\"AT_descriptors2.csv\")\n",
    "def distance_PP2(network_name):\n",
    "\n",
    "    df = pd.read_csv(network_name + \"_descriptors.csv\")\n",
    "    df1 = df.drop(columns = ['Unnamed: 0','AMP_Name'])\n",
    "    #df1.index = df['AMP_Name']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_normalized = scaler.fit_transform(df1)\n",
    "    scaled_features_df = pd.DataFrame(df_normalized, index=df1.index, columns=df1.columns)\n",
    "    scaled_features_df['AMP_Name']=df['AMP_Name']\n",
    "    scaled_features_df.set_index(\"AMP_Name\", inplace = True)\n",
    "    ll=df['AMP_Name'].tolist()\n",
    "    \n",
    "    with open(path_h+network_name+'_pair.txt') as file:\n",
    "        lines = [line.rstrip() for line in file]\n",
    "            \n",
    "        P1_list=[]\n",
    "        P2_list=[]\n",
    "        PPI_list=[]\n",
    "        PPI_dict={}\n",
    "        for items in lines:\n",
    "            x = items.split(\"\\t\")\n",
    "            #print(x)\n",
    "            if x[0] in ll:\n",
    "                if x[1] in ll:\n",
    "                    if x[2]=='+':\n",
    "                        PPI_dict[x[0],x[1]]=1\n",
    "                    else:\n",
    "                        PPI_dict[x[0],x[1]]=0    \n",
    "            \n",
    "    \n",
    "    # construct distance matrix for PPI\n",
    "    df_distance= pd.DataFrame()\n",
    "    df_distance['PPI_names']=list(PPI_dict.keys())\n",
    "    df_distance['PPI']=list(PPI_dict.values())\n",
    "\n",
    "    for columns in df1:\n",
    "        scaled_features_df2=scaled_features_df[columns]\n",
    "        xx=[]\n",
    "        for x , y in PPI_dict.items():\n",
    "            amp_distance=Euclidean_distance (scaled_features_df2, x[0], x[1])\n",
    "            xx.append(amp_distance)\n",
    "        df_distance[columns]=xx\n",
    "    #print(df_distance)\n",
    "    \n",
    "    df_distance.to_csv(network_name+'_distance.csv')\n",
    "    \n",
    "    return df_distance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network_data(network_name, path_h):\n",
    "    \n",
    "    df0 = pd.read_csv(path_h + network_name + \"_distance.csv\")\n",
    "    df = df0.drop(columns=['Unnamed: 0'])\n",
    "    df2 = df.drop(columns=['PPI_names'])\n",
    "    df3 = df.drop(columns=['PPI_names', 'PPI'])\n",
    "    \n",
    "    XX = np.array(df3)\n",
    "    yy = np.array(df2['PPI'])\n",
    "    X=np.array(XX)\n",
    "    y=np.array(yy)\n",
    "    klist = np.array(df3.columns)\n",
    "    \n",
    "    return X, y,klist,df3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
